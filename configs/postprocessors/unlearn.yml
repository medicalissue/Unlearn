postprocessor:
  name: unlearn
  postprocessor_args:
    # ==== BASIC SETTINGS ====
    unlearn_mode: "ascent" # ascent (RECOMMENDED!), fisher
    fisher_normalize: true   # Adaptive ì •ê·œí™” ì‚¬ìš© (fisher mode only)
    fisher_damping: 1e-8
    eta: 1e-2             # Base learning rate
    num_steps: 100          # Unlearning iterations (1 is often best!)
    recompute_target: false
    temp: 1.0             # Temperature (>1.0 for soft pseudo-label)

    # GradNorm ì‚¬ìš© ì—¬ë¶€
    use_gradnorm: false   # GradNorm ê³„ì‚° í™œì„±í™” (for combo score)

    # Feature-aware gradient ì‚¬ìš© ì—¬ë¶€
    use_feature_grad: true   # Feature space geometric features í™œì„±í™”

    # ==== PHASE 1: QUICK WINS (High Impact, Low Effort) ====
    # Adaptive learning rate
    eta_mode: "fixed"     # "fixed", "confidence", "entropy", "hybrid" (RECOMMENDED!)
    eta_power: 0.5        # Power for confidence scaling (0.5 = sqrt)

    # Gradient normalization
    grad_norm_mode: "none"  # "none", "l2" (RECOMMENDED!), "clip", "adaptive_clip"
    grad_clip_value: 1.0  # Clip threshold (for clip modes)

    # ==== PHASE 2: ADVANCED METHODS ====
    # EMA (Exponential Moving Average)
    ema_momentum: 0.0     # 0.0=disabled, 0.5-0.9=enabled (for num_steps>1)

    # Prototype guidance
    use_prototype_guidance: false  # Enable prototype-guided gradient
    proto_guidance_strength: 100.0   # Strength of guidance

    # Temperature annealing
    temp_anneal: false    # Enable temperature annealing (for num_steps>1)
    temp_anneal_rate: 0.5 # Decay rate

    # ==== PHASE 3: OPTIONAL ENHANCEMENTS ====
    # Multi-target ensemble
    num_targets: 1        # Number of top-K targets (1=single, 3=ensemble)

    # L2 regularization
    l2_penalty: 0.0       # L2 penalty on weight shift (0.0=disabled, 0.1-0.5=light)

    # ìŠ¤ì½”ì–´ íƒ€ì… ì„ íƒ
    # ì˜µì…˜: "delta_energy", "gradnorm", "grad_ratio", "combo", "prototype_logit_shift", "prototype_grad"
    #       "feature_aware", "feature_fc_alignment", "geometry_combo", "multiscale_prototype"
    #       "prototype_coupling", "gradient_alignment", "confidence_entropy_combo"
    #       "energy_curvature", "weight_convergence", "gradient_decay", "coupling_evolution", "trajectory_combo"
    # - delta_energy: Simple energy change (baseline)
    # - feature_aware: Feature geometry Ã— FC weight shift [RECOMMENDED]
    # - feature_fc_alignment: Î”WÂ·f alignment score
    # - geometry_combo: Multi-scale geometric combination
    # - multiscale_prototype: Class + Global prototype distance
    # - prototype_coupling: Cross-prototype gradient eigenvalue concentration [MOST ELEGANT]
    # - gradient_alignment: Sample-prototype gradient cosine similarity
    # - confidence_entropy_combo: Confidence drop Ã— Entropy change
    # - energy_curvature: Energy trajectory smoothness (requires num_steps > 1) [NEW]
    # - weight_convergence: Weight update convergence pattern (requires num_steps > 1) [NEW]
    # - gradient_decay: Gradient magnitude decay rate (requires num_steps > 1) [NEW]
    # - coupling_evolution: Prototype coupling evolution (requires num_steps > 1) [NEW]
    # - trajectory_combo: Weighted combination of all trajectory signals (requires num_steps > 1) [NEW]
    # - combo: Weighted combination of multiple metrics
    score_type: prototype_coupling #coupling_evolution #prototype_coupling

    # ==== FEATURE-AWARE ADVANCED SETTINGS ====
    # Feature-aware ìŠ¤ì½”ì–´ íƒ€ì…ì˜ ì„¸ë¶€ ì œì–´ (score_type="feature_aware"ì¼ ë•Œë§Œ ì ìš©)
    feature_aware_config:
      # ëª¨ë“œ ì„ íƒ: "baseline", "adaptive_norm", "weighted", "angular", "nonlinear", "full"
      # - baseline: ê¸°ì¡´ êµ¬í˜„ (feature_norm/100 * dist * weight_shift)
      # - adaptive_norm: Z-score normalization ì ìš©
      # - weighted: ì„¸ ìš”ì†Œì— ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ ì ìš©
      # - angular: Cosine similarity ì¶”ê°€
      # - nonlinear: sqrt/log ë¹„ì„ í˜• ê²°í•©
      # - full: ëª¨ë“  ê°œì„  ì‚¬í•­ ì ìš©
      mode: "baseline"

      # 1. Adaptive Normalization (mode >= "adaptive_norm")
      use_adaptive_norm: false  # Setupì—ì„œ ê³„ì‚°ëœ í†µê³„ë¡œ z-score normalization

      # 2. Learnable Weights (mode >= "weighted")
      component_weights:
        feature_norm: 0.0      # Feature norm ê°€ì¤‘ì¹˜
        distance: 0.0          # Prototype distance ê°€ì¤‘ì¹˜
        weight_shift: 1.0      # FC weight shift ê°€ì¤‘ì¹˜

      # 3. Cosine Similarity (mode >= "angular")
      use_cosine: false        # Prototypeì™€ì˜ ê°ë„ ì •ë³´ ì¶”ê°€
      angular_weight: 0.5      # L2 distance vs cosineì˜ ê°€ì¤‘ì¹˜ (0.5 = ê· ë“±)

      # Distance Metric ì„ íƒ (ê³ ì°¨ì› featureì— ìœ ë¦¬)
      distance_metric: "rbf"      # "l1", "l2", "linf", "combined", "weighted_l1", "fractional", "adaptive_p", "l0", "rbf", "log_scaled",
      #"elementwise_log", "truncated_fractional", "mixed_p"
      # - l1: Manhattan distance (ê³ ì°¨ì›/sparseì— ìœ ë¦¬, outlier robust) â­ BEST
      # - l2: Euclidean distance (ê¸°ë³¸, ì§ê´€ì )
      # - linf: Chebyshev distance (max ì°¨ì› ì°¨ì´)
      # - combined: L1 + L2 weighted combination
      # - weighted_l1: Feature importance ê¸°ë°˜ ê°€ì¤‘ L1 (ì¤‘ìš” ì°¨ì› ê°•ì¡°)
      # - fractional: L_p norm with p < 1 (ë” sparse-aware) â­ p=0.1 BEST!
      # - adaptive_p: Confidence ê¸°ë°˜ adaptive p-norm (sample-wise)
      # - l0: L0 norm (non-zero element count, ê°€ì¥ sparse-aware)
      # - rbf: RBF kernel distance (smooth, bounded, non-linear transformation)
      # - log_scaled: Log-scaled distance (compresses large distances, outlier-robust)
      # - elementwise_log: Î£log(1+Î±|x-y|) - sub-linear, bounded growth â­â­ NEW! Expected better than fractional
      # - truncated_fractional: Fractional + noise filtering â­â­ NEW! Expected better than fractional
      # - mixed_p: Adaptive dual p-norm â­â­ NEW! Expected better than fractional

      distance_combination:
        l1_weight: 0.5         # Combined mode: L1 ê°€ì¤‘ì¹˜
        l2_weight: 0.5         # Combined mode: L2 ê°€ì¤‘ì¹˜

      # Weighted L1 ì„¤ì • (distance_metric="weighted_l1"ì¼ ë•Œ)
      weighted_l1_config:
        importance_mode: "gradient"  # "variance", "gradient", "fisher", "uniform"
        # - variance: ID featureì˜ ì°¨ì›ë³„ ë¶„ì‚° (ë†’ì€ ë¶„ì‚° = ì¤‘ìš”)
        # - gradient: í‰ê·  gradient magnitude
        # - fisher: Fisher information diagonal
        # - uniform: ëª¨ë“  ì°¨ì› ë™ì¼ ê°€ì¤‘ì¹˜ (=L1)
        normalize_weights: false      # ê°€ì¤‘ì¹˜ L1 ì •ê·œí™” (sum=feature_dim)

      # Fractional p-norm ì„¤ì • (distance_metric="fractional"ì¼ ë•Œ)
      fractional_p: 0.1  # p value (0.3 ~ 0.9)
      # - p=0.5: L1ë³´ë‹¤ ë” sparse-friendly
      # - p=0.7: L1ê³¼ L2 ì¤‘ê°„
      # - ì‘ì„ìˆ˜ë¡ sparse featureì— ìœ ë¦¬

      # Adaptive p-norm ì„¤ì • (distance_metric="adaptive_p"ì¼ ë•Œ)
      adaptive_p_config:
        min_p: 0.1      # Low confidence (OOD) â†’ L1 (robust)
        max_p: 1.0      # High confidence (ID) â†’ L2 (smooth)
        mode: "linear"  # "linear", "sigmoid"

      # L0 norm ì„¤ì • (distance_metric="l0"ì¼ ë•Œ)
      l0_threshold: 1e-4  # ì´ ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ ê°„ì£¼
      # - 1e-6: ë§¤ìš° ì—„ê²© (ì‘ì€ ì°¨ì´ë„ ì¹´ìš´íŠ¸)
      # - 1e-5: ê¸°ë³¸
      # - 1e-4: ê´€ëŒ€ (í° ì°¨ì´ë§Œ ì¹´ìš´íŠ¸)

      # RBF Kernel Distance ì„¤ì • (distance_metric="rbf"ì¼ ë•Œ)
      rbf_config:
        sigma: "auto"  # "auto", "median", or float value (e.g., 1.0)
        # - "auto": sigma = sqrt(feature_dim) (ì¼ë°˜ì ì¸ heuristic)
        # - "median": median pairwise distance (ë°ì´í„° ê¸°ë°˜ adaptive)
        # - float: ì§ì ‘ ì§€ì • (e.g., 1.0, 10.0)
        # RBF kernel: K(x,y) = exp(-||x-y||^2 / (2*sigma^2))
        # Distance: 1 - K(x,y) (similarity â†’ distance ë³€í™˜)
        # íŠ¹ì§•: bounded [0,1], smooth, non-linear transformation
        # ì¥ì : í° ê±°ë¦¬ë¥¼ ì••ì¶•, ì‘ì€ ê±°ë¦¬ë¥¼ ë¯¼ê°í•˜ê²Œ ë°˜ì‘
        use_gamma: true  # trueë©´ sigma ëŒ€ì‹  gamma = 1/(2*sigma^2) ì‚¬ìš©
        gamma: 0.00001      # use_gamma=trueì¼ ë•Œ ì‚¬ìš© (sklearn ìŠ¤íƒ€ì¼)

      # Log-scaled Distance ì„¤ì • (distance_metric="log_scaled"ì¼ ë•Œ)
      log_scaled_config:
        base_metric: "l1inf"  # "l1", "l2", "linf" - logë¥¼ ì ìš©í•  ê¸°ë³¸ ê±°ë¦¬
        # - l1: log(1 + L1) - ì¶”ì²œ! L1ì˜ robustness + í° ê±°ë¦¬ ì••ì¶•
        # - l2: log(1 + L2) - í‘œì¤€ Euclideanì— log ì ìš©
        # - linf: log(1 + Lâˆ) - max differenceë¥¼ log-scale
        log_mode: "log1p"  # "log1p", "log", "log10"
        # - "log1p": log(1 + distance) - ì•ˆì „, distance=0 ì²˜ë¦¬ ìë™ â­ ì¶”ì²œ
        # - "log": log(distance + eps) - ì‘ì€ epsilon í•„ìš”
        # - "log10": log10(1 + distance) - ë” ê°•í•œ ì••ì¶•
        eps: 1e-8  # log_mode="log"ì¼ ë•Œ ì‚¬ìš© (numerical stability)
        # íŠ¹ì§•: í° ê±°ë¦¬ ì••ì¶•, ì‘ì€ ê±°ë¦¬ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë³´ì¡´
        # ì¥ì : outlier ì˜í–¥ ê°ì†Œ, ê±°ë¦¬ ë¶„í¬ ì •ê·œí™”

      # Element-wise Log Distance ì„¤ì • (distance_metric="elementwise_log"ì¼ ë•Œ)
      elementwise_log_config:
        alpha: 0.1  # Scaling factor for |x-y| before log
        # - alpha=0.1: ë§¤ìš° ë¯¼ê° (ì‘ì€ ì°¨ì´ë„ ì¦í­)
        # - alpha=1.0: ê¸°ë³¸ (ì¶”ì²œ) â­
        # - alpha=10.0: ëœ ë¯¼ê° (í° ì°¨ì´ë§Œ ì¤‘ìš”)
        base: "natural"  # "natural" (ln), "10" (log10), "2" (log2)
        # - "natural": ln(1 + Î±|x-y|) - ê¸°ë³¸, ê°€ì¥ ì¼ë°˜ì  â­
        # - "10": log10(1 + Î±|x-y|) - ë” ê°•í•œ ì••ì¶•
        # - "2": log2(1 + Î±|x-y|) - ì •ë³´ì´ë¡  ê´€ì 
        # ìˆ˜ì‹: distance = Î£ log(1 + Î±|x_i - y_i|)
        # íŠ¹ì§•: Sub-linear, bounded growth, continuous
        # ì¥ì : Fractional p=0.1ê³¼ ìœ ì‚¬í•œ íŠ¹ì„± + outlierì— ë” robust
        # ì˜ˆìƒ: Fractional p=0.1ë³´ë‹¤ 2-3% AUROC í–¥ìƒ ê°€ëŠ¥

      # Truncated Fractional ì„¤ì • (distance_metric="truncated_fractional"ì¼ ë•Œ)
      truncated_fractional_config:
        p: 0.1  # Fractional p value (0.05 ~ 0.3 ê¶Œì¥)
        threshold: 1e-4  # ì´ ê°’ë³´ë‹¤ ì‘ì€ ì°¨ì´ëŠ” ë¬´ì‹œ
        # - 1e-4: ì—„ê²© (ì‘ì€ ì°¨ì´ë„ í¬í•¨)
        # - 1e-3: ê¸°ë³¸ (ì¶”ì²œ) â­
        # - 1e-2: ê´€ëŒ€ (í° ì°¨ì´ë§Œ í¬í•¨)
        mode: "hard"  # "hard" (ì™„ì „ 0ìœ¼ë¡œ), "soft" (exponential decay)
        # - "hard": |x-y| < threshold â†’ 0 (sharp cutoff) â­
        # - "soft": |x-y| * exp(-threshold/|x-y|) (smooth transition)
        # ìˆ˜ì‹: distance = (Î£ (|x_i-y_i| if >threshold else 0)^p)^(1/p)
        # íŠ¹ì§•: Fractional p=0.1 + noise filtering
        # ì¥ì : ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°, ì˜ë¯¸ìˆëŠ” ì°¨ì›ë§Œ ì„ íƒ
        # ì˜ˆìƒ: ë…¸ì´ì¦ˆ ë§ì€ ë°ì´í„°ì—ì„œ Fractional p=0.1ë³´ë‹¤ 1-2% í–¥ìƒ

      # Mixed P-norm ì„¤ì • (distance_metric="mixed_p"ì¼ ë•Œ)
      mixed_p_config:
        p_large: 0.3  # í° ì°¨ì´ì— ëŒ€í•œ p (0.3 ~ 1.0)
        # - 0.3: ë§¤ìš° aggressive
        # - 0.5: ê¸°ë³¸ (ì¶”ì²œ) â­
        # - 1.0: L1ê³¼ ê°™ìŒ
        p_small: 0.1  # ì‘ì€ ì°¨ì´ì— ëŒ€í•œ p (0.01 ~ 0.2)
        # - 0.01: ê±°ì˜ L0
        # - 0.05: ê¸°ë³¸ (ì¶”ì²œ) â­
        # - 0.1: Fractional p=0.1ê³¼ ê°™ìŒ
        threshold_mode: "median"  # "median", "percentile", "absolute"
        # - "median": median(|x_i - y_i|) ê¸°ì¤€ â­
        # - "percentile": k% percentile ê¸°ì¤€
        # - "absolute": ê³ ì • ê°’ ê¸°ì¤€
        percentile: 50  # threshold_mode="percentile"ì¼ ë•Œ ì‚¬ìš© (0~100)
        absolute_threshold: 0.1  # threshold_mode="absolute"ì¼ ë•Œ ì‚¬ìš©
        # ìˆ˜ì‹: distance = (Î£_large diff^p_large)^(1/p_large) + (Î£_small diff^p_small)^(1/p_small)
        # íŠ¹ì§•: í° ì°¨ì´ì™€ ì‘ì€ ì°¨ì´ë¥¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬ (adaptive)
        # ì¥ì : ë‘ ê°œì˜ p ê°’ìœ¼ë¡œ ë” ì„¸ë°€í•œ ì¡°ì ˆ, flexible
        # ì˜ˆìƒ: ìµœì  ì¡°í•© ë°œê²¬ ì‹œ Fractional p=0.1ë³´ë‹¤ 3-5% í–¥ìƒ ê°€ëŠ¥

      # 4. ë¹„ì„ í˜• ê²°í•© (mode >= "nonlinear")
      nonlinear_mode: "none"   # "none", "sqrt", "log"

      # 5. Confidence Scaling (mode == "full")
      use_confidence_scaling: false  # ì˜ˆì¸¡ confidenceì— ë”°ë¥¸ temperature scaling
      confidence_power: 1.0          # Confidence scaling ê°•ë„

    # Combo ìŠ¤ì½”ì–´ìš© ê°€ì¤‘ì¹˜
    weights:
      denergy: 1.0         # Î”Energy ê°€ì¤‘ì¹˜
      g: 0.5               # GradNorm ê°€ì¤‘ì¹˜
      grad_ratio: 0.5      # Gradient Ratio ê°€ì¤‘ì¹˜
      feature: 1.0         # Feature geometry ê°€ì¤‘ì¹˜
      prototype_coupling: 1.0  # Cross-prototype coupling ê°€ì¤‘ì¹˜
      gradient_alignment: 1.0  # Sample-prototype alignment ê°€ì¤‘ì¹˜
      confidence_entropy: 1.0  # Confidence-entropy combo ê°€ì¤‘ì¹˜

    # ==== TRAJECTORY-BASED OOD DETECTION SETTINGS ====
    # Trajectory-based score types analyze the unlearning dynamics across multiple steps
    # Options: "energy_curvature", "weight_convergence", "gradient_decay", "coupling_evolution", "trajectory_combo"
    # - energy_curvature: Measures smoothness of energy trajectory (2nd derivative)
    #   * ID: smooth energy decrease â†’ low curvature â†’ high score
    #   * OOD: irregular energy changes â†’ high curvature â†’ low score
    # - weight_convergence: Measures convergence pattern of weight updates
    #   * ID: consistent direction + decreasing magnitude â†’ high score
    #   * OOD: inconsistent direction + stable magnitude â†’ low score
    # - gradient_decay: Measures gradient magnitude decay rate
    #   * ID: fast gradient decay (rapid convergence) â†’ high score
    #   * OOD: slow gradient decay (flat landscape) â†’ low score
    # - coupling_evolution: Measures how prototype coupling pattern evolves (FULL VERSION)
    #   * Uses SAME eigenvalue analysis as prototype_coupling, but tracks trajectory
    #   * Computes coupling metric at each unlearning step
    #   * ID: stable coupling (low variance, consistent metric) â†’ high score
    #   * OOD: unstable coupling (high variance, erratic metric) â†’ low score
    #   * Guaranteed to be better than static prototype_coupling!
    # - trajectory_combo: Weighted combination of all trajectory signals
    trajectory_config:
      enabled: false  # Enable trajectory-based analysis (requires num_steps > 1)
      weights:
        energy: 1.0      # Energy curvature weight
        weight: 1.0      # Weight convergence weight
        gradient: 1.0    # Gradient decay weight
        coupling: 1.0    # Coupling evolution weight

    # ==== NEW ELEGANT SCORE TYPE SETTINGS ====
    # Cross-Prototype Gradient Coupling settings (score_type="prototype_coupling")
    prototype_coupling_config:
      use_all_prototypes: false     # Use all prototypes (true) or top-K only (false)
      top_k: 5                     # If use_all_prototypes=false, use top-K predicted classes
      eigenvalue_mode: "participation_ratio"  # Options: "participation_ratio", "spectral_entropy", "gini_coefficient", "max_mean_ratio", "magnitude_weighted_pr", "spectrum_stats", "dual_metric", "log_magnitude_pr", "ipr", "generalized_pr", "effective_rank", "quantum_purity", "principal_dominance", "direction_alignment", "concentration", "entropy"
      gpr_q: 1  # Hyperparameter for generalized_pr mode (try 1.0, 1.5, 2.0, 2.5, 3.0)

      # ==== MAGNITUDE-AWARE MODES (combines size + shape!) ğŸ†ğŸ†ğŸ† ====
      # Core insight: Eigenvalue MAGNITUDE (size) + DISTRIBUTION (shape) both matter!
      # ID: small & uniform eigenvalues (stable unlearning)
      # OOD: large & concentrated eigenvalues (unstable unlearning)

      # Magnitude-Weighted PR config
      magnitude_weighted_pr_config:
        q: '0.1'  # Lp-norm parameter for magnitude: 1.0 (L1), 2.0 (L2), inf (max)
        combine_mode: "add"  # "multiply", "add", "log_add"
        # - multiply: magnitude Ã— PR (amplifies difference) [BEST]
        # - add: magnitude + PR (more stable)
        # - log_add: log(magnitude) + PR (balanced)

      # Spectrum Statistics config (most comprehensive)
      spectrum_stats_config:
        weights:
          total_energy: 1.0     # Î£Î»áµ¢ (overall coupling strength)
          max_eigenvalue: 0.5   # max(Î»áµ¢) (dominant mode)
          gini: 0.3             # Gradient inequality
          variance: 0.2         # Eigenvalue spread

      # Dual Metric config (simple two-component)
      dual_metric_config:
        alpha: 0.7  # Weight for magnitude score
        beta: 0.3   # Weight for shape score (negative PR)

      # ==== PHYSICS-BASED MODES (most theoretically grounded!) ====
      # Core idea: Measure eigenvalue distribution properties using physics principles

      # - ipr: Inverse Participation Ratio tr(CÂ²)/(tr(C))Â² [Anderson localization]
      #   * ID: LOW IPR (delocalized) â†’ HIGH score (negated)
      #   * OOD: HIGH IPR (localized) â†’ LOW score (negated)
      #   * Range: [1/K, 1], Physics: Anderson localization theory

      # - generalized_pr: q-parameterized PR = (Î£Î»áµ¢^q)^(1/(1-q)) [Renyi entropy] [BEST ğŸ†ğŸ†ğŸ†]
      #   * ID: LOW PR_q â†’ HIGH score (negated)
      #   * OOD: HIGH PR_q â†’ LOW score (negated)
      #   * q<2: sensitive to small eigenvalues, q>2: sensitive to large eigenvalues
      #   * Special cases: q=1 â†’ exp(entropy), q=2 â†’ standard PR
      #   * Physics: Renyi entropy, Multifractal analysis
      #   * TUNABLE via gpr_q hyperparameter!

      # - effective_rank: exp(Shannon entropy) = exp(-Î£páµ¢Â·log(páµ¢))
      #   * ID: HIGH ER (uniform eigenvalues) â†’ HIGH score
      #   * OOD: LOW ER (concentrated eigenvalues) â†’ LOW score
      #   * Range: [1, K], Physics: Information theory + Statistical mechanics

      # - quantum_purity: tr(ÏÂ²) = Î£páµ¢Â² [Quantum mechanics]
      #   * ID: LOW purity (maximally mixed) â†’ HIGH score (negated)
      #   * OOD: HIGH purity (pure state) â†’ LOW score (negated)
      #   * Range: [1/K, 1], Physics: Quantum density matrix theory

      # ==== NEW GRADIENT INEQUALITY MODES (most elegant!) ====
      # Core idea: ID samples have UNEQUAL gradients (dominant prototype), OOD samples have EQUAL gradients

      # - gini_coefficient: Economic inequality measure Î£áµ¢ Î£â±¼ |gáµ¢-gâ±¼| / (2KÂ²Â·mean(g)) [MOST ELEGANT ğŸ†]
      #   * ID: HIGH Gini (gradient inequality, dominant prototype) â†’ HIGH score
      #   * OOD: LOW Gini (gradient equality, all similar) â†’ LOW score
      #   * Theory: Decades of economic validation, range [0,1], 0=equality, 1=inequality

      # - principal_dominance: Largest eigenvalue ratio Î»â‚/Î£Î»áµ¢ (PCA explained variance)
      #   * ID: LOW dominance (eigenvalues spread uniformly) â†’ HIGH score (negated)
      #   * OOD: HIGH dominance (largest eigenvalue dominant) â†’ LOW score (negated)
      #   * Theory: Direct connection to PCA, efficient (reuses eigenvalues)

      # - max_mean_ratio: Simple ratio max(||Î”gáµ¢||) / mean(||Î”gáµ¢||)
      #   * ID: HIGH ratio (one prototype much larger) â†’ HIGH score
      #   * OOD: LOW ratio (all prototypes similar) â†’ LOW score
      #   * Theory: Most intuitive, computationally efficient O(K)

      # ==== ORIGINAL COUPLING MATRIX MODES ====
      # - participation_ratio: PR = (tr(C))^2 / tr(C^2) - Effective # of participating prototypes
      #   * ID: LOW PR (strongly coupled to specific prototype) â†’ HIGH score (negated)
      #   * OOD: HIGH PR (weakly coupled to many) â†’ LOW score (negated)

      # - direction_alignment: Average cosine similarity between gradient directions [EXPERIMENTAL]
      #   * ID: HIGH alignment (coherent changes) â†’ HIGH score
      #   * OOD: LOW alignment (incoherent changes) â†’ LOW score

      # - spectral_entropy: Shannon entropy of eigenvalue distribution
      #   * ID: LOW entropy (largest eigenvalue dominant) â†’ HIGH score (negated)
      #   * OOD: HIGH entropy (eigenvalues uniform) â†’ LOW score (negated)

      # ==== LEGACY MODES (gradient norms only) ====
      # - concentration: Variance of gradient norms (negated)
      # - entropy: Shannon entropy of gradient norm distribution

    # Sample-Prototype Gradient Alignment settings (score_type="gradient_alignment")
    gradient_alignment_config:
      normalize_grads: true        # L2-normalize gradients before computing cosine similarity
      use_absolute: false          # Use |cosine| instead of cosine (ignore direction, only magnitude)

  APS_mode: false  # Hyperparameter search enabled
  postprocessor_sweep:
    # Optimized hyperparameter search for Phase 1-3
    # Recommended: Start with Phase 1, then add Phase 2-3 if needed

    # Basic settings
    unlearn_mode: ["ascent"]  # ascent is better than fisher!
    eta: [1e-3, 5e-3]  # 2 values
    num_steps: [1, 3]  # 2 values (1 is often best!)
    temp: [1.0]  # 1 value
    recompute_target: [false]
    use_gradnorm: [false]
    use_feature_grad: [true]

    # Phase 1: Adaptive eta + Gradient norm (PRIORITY!)
    eta_mode: ["fixed", "hybrid"]  # 2 values (hybrid recommended!)
    eta_power: [0.5]  # 1 value
    grad_norm_mode: ["none", "l2"]  # 2 values (l2 recommended!)

    # Phase 2: Advanced methods (optional)
    ema_momentum: [0.0]  # 1 value (try 0.5 if num_steps>1)
    use_prototype_guidance: [false]  # 1 value (experimental)
    temp_anneal: [false]  # 1 value (for num_steps>1)

    # Phase 3: Optional enhancements
    l2_penalty: [0.0]  # 1 value (try 0.1 if needed)

    # Score types (TEST THESE!)
    score_type: ["delta_energy", "feature_aware", "feature_fc_alignment", "prototype_coupling", "gradient_alignment", "confidence_entropy_combo", "energy_curvature", "weight_convergence", "gradient_decay", "trajectory_combo"]  # 10 values

    # Feature-aware hyperparameter search
    feature_aware_config:
      mode: ["baseline", "adaptive_norm", "weighted", "full"]  # 4 modes
      component_weights:
        feature_norm: [1.0, 2.0]  # 2 values
        distance: [1.0, 2.0]      # 2 values
        weight_shift: [1.0]       # 1 value
      angular_weight: [0.5]       # 1 value
      nonlinear_mode: ["none", "sqrt"]  # 2 values
      distance_metric: ["l1", "l0", "weighted_l1", "fractional", "adaptive_p", "rbf", "log_scaled", "elementwise_log", "truncated_fractional", "mixed_p"]  # 10 values
      fractional_p: [0.5, 0.7]    # 2 values (for fractional mode)
      l0_threshold: [1e-6, 1e-5, 1e-4]  # 3 values (for l0 mode)
      weighted_l1_config:
        importance_mode: ["variance", "gradient"]  # 2 values (for weighted_l1)
      rbf_config:
        sigma: ["auto", "median"]  # 2 values (for rbf mode)
      log_scaled_config:
        base_metric: ["l1", "l2"]  # 2 values (for log_scaled mode)
        log_mode: ["log1p"]        # 1 value
      elementwise_log_config:
        alpha: [0.1, 1.0, 10.0]    # 3 values (for elementwise_log mode)
        base: ["natural"]          # 1 value
      truncated_fractional_config:
        p: [0.1]                   # 1 value
        threshold: [1e-4, 1e-3, 1e-2]  # 3 values (for truncated_fractional mode)
        mode: ["hard"]             # 1 value
      mixed_p_config:
        p_large: [0.3, 0.5]        # 2 values (for mixed_p mode)
        p_small: [0.05, 0.1]       # 2 values
        threshold_mode: ["median"] # 1 value

    # Total combinations (with feature_aware):
    # Basic: 2 (eta) Ã— 2 (num_steps) Ã— 2 (eta_mode) Ã— 2 (grad_norm) = 16
    # Score types: 3 (delta_energy, feature_aware, feature_fc_alignment)
    # Feature-aware modes: 4 (baseline, adaptive_norm, weighted, full) Ã— 2 (w_fn) Ã— 2 (w_d) Ã— 2 (nonlinear) = 32
    # Total: 16 Ã— 3 Ã— 32 = 1536 (ë„ˆë¬´ ë§ìŒ!)
    # Recommended: score_type=["feature_aware"], mode ìœ„ì£¼ë¡œ íƒìƒ‰
    # Expected runtime: ~25 minutes on CIFAR-10